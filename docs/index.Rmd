---
title: "Final Project - Biometry II"
author: "Niko Carvajal Janke"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}


```

------------------------------------------------------------------------

## Table Of Contents

-   **Introduction**
    -   Goals for Regression
    -   Dataset Selection
-   **Analysis**
    -   Exploratory Data Analysis
    -   Visualizing Predictors
    -   Fitting & Tuning A Linear Model
    -   Switching To A Mixed-Effects Model
    -   Can Lasso or Elastic Net Improve Model Performance?
    -   Comparing RMSE and MAE of the Three Models
-   **Discussion & Conclusions**
    -   Mixed-Effects Model: Feature Impact Analysis & Hypothesis Testing
    -   Dataset Challenges and Limitations of Results
-   **References**

------------------------------------------------------------------------

# Introduction

### Goals for Regression

I am interested in determining if I can build a model to identify if certain student lifestyle variables will significantly impact the average daily usage hours of screen time of that individual. Specifically, I aim to evaluate the influence of factors such as Age, Gender, Academic Level, Country, Most Used Social Media Platform, Sleep Hours Per Night, Relationship Status, Frequency of Conflicts over Social Media, and self-reported metrics including Social Media Addiction Score, Mental Health Score, and if Social Media Affects Academic Performance (Yes/No).

### Dataset Selection

The dataset I chose is called **Students' Social Media Addiction** and can be accessed here: <https://www.kaggle.com/datasets/adilshamim8/social-media-addiction-vs-relationships/data>, and is also available in this github repo as "Students Social Media Addiction". Below is a variable description provided by Adil Shamim, who compiled the dataset in 2025. Further information about the methods for data collection can be accessed at the linked kaggle page.

-   **Student_ID** *(Integer)*: Unique respondent identifier
-   **Age** *(Integer)*: Age in years
-   **Gender** *(Categorical)*: “Male” or “Female”
-   **Academic_Level** *(Categorical)*: High School / Undergraduate / Graduate
-   **Country** *(Categorical)*: Country of residence
-   **Avg_Daily_Usage_Hours** *(Float)*: Average hours per day on social media
-   **Most_Used_Platform** *(Categorical)*: Instagram, Facebook, TikTok, etc.
-   **Affects_Academic_Performance** *(Boolean)*: Self‐reported impact on academics (Yes/No)
-   **Sleep_Hours_Per_Night** *(Float)*: Average nightly sleep hours
-   **Mental_Health_Score** *(Integer)*: Self‐rated mental health (1 = poor to 10 = excellent)
-   **Relationship_Status** *(Categorical)*: Single / In Relationship / Complicated
-   **Conflicts_Over_Social_Media** *(Integer)*: Number of relationship conflicts due to social media
-   **Addicted_Score** *(Integer)*: Social Media Addiction Score (1 = low to 10 = high)

------------------------------------------------------------------------

# Analysis

### Exploratory Data Analysis

Below is the first series of steps, where I download the dataset of interest, complete **summary statistics of variables, visualize variable distributions and relationships, look for missing values and outliers,** and **clean and preprocess the data.**

```{r Package Loading, message=FALSE, warning=FALSE}
#Load all the relevant packages for this project
library(skimr)
library(tidyverse)
library(car)
library(leaps)
library(glmnet)
library(lmtest)
library(lme4)
```

```{r Importing Dataset, Preliminary Assessment, and Cleaning}

#import dataset. THE USER MUST SELECT THEIR OWN LOCATION OF THE DATASET

rawdata<- read.csv(file.choose())

#summary statistics of variables | no missing values
skim(rawdata)


#checking for duplicate values
table(rawdata$Student_ID)[table(rawdata$Student_ID) > 1]

#Removing StudentID since it is redundant
rawdata<-rawdata%>%
  select(!Student_ID)





```

## Visualizing Predictors

Next I visualize each of the predictor variables to get a sense of distributions that may be a problem later on if not addressed. While variables like sleep hours looks abnormal, the continuous and discrete data look ok besides that. We will see how the assumptions fair. The categorical variables look ok.

```{r Visualizing predictors, fig.height=4, fig.width=6, paged.print=TRUE}

##Visualizing distributions of numeric variables & their relation to daily usage

numeric_vars <- names(rawdata)[sapply(rawdata, is.numeric)]

for (var in numeric_vars) {
  hist(rawdata[[var]],
       main = var)
  plot(rawdata$Avg_Daily_Usage_Hours, rawdata[[var]],  main = var)
}

#Sleep hours per night looks abnormal, but not sure what to do. No crazy outliers

##Visualizing relationships of categorical variables to daily usage

cat_vars <- names(rawdata)[sapply(rawdata, function(x) !is.numeric(x) & !is.logical(x))]

# Loop through categorical variables and create boxplots
for (var in cat_vars) {
  boxplot(rawdata$Avg_Daily_Usage_Hours ~ rawdata[[var]],
          main = paste("Average Daily Usage by", var),
          ylab = "Avg Daily Usage (Hours)",
          xlab = var)
}


#Categorical variables seem to vary in their likely importance. 
```

## Fitting & Tuning a Linear Model (1/3)

Now I will fit a linear model to identify what strategies may be appropriate for linear regression using this dataset.

I will **verify regression assumptions** such as **linearity, normality, homoscedasticity, independence,** and **multicollinearity.** I will also **address any assumption violations and corrections** necessary. The first model shows that there are likely significant relationships to be found between our predictor variables and daily usage hours. However, there is evidence for multicollinearity, non-independence, and non-normality, which I initially address by removing addicted score since it is likely too correlated with daily usage hours to provide valuable inference for the other variables.

```{r Fitting initial LM with initial assumption assessment and transformations, paged.print=FALSE}

##First we will fit a basic linear model to compare to:

m1 <- lm(Avg_Daily_Usage_Hours ~ Age + Gender + Country + Academic_Level + 
              Most_Used_Platform + Affects_Academic_Performance + 
              Sleep_Hours_Per_Night + Mental_Health_Score + 
              Relationship_Status + Conflicts_Over_Social_Media + 
              Addicted_Score,
            data = rawdata)
summary(m1)

##Checking for multicolinearity | FAIL (Addicted Score)
vif(m1)

#Checking for residual independence | FAIL
dwtest(m1)

## Checking for Normality of residuals | FAIL

qqnorm(m1$residuals)
qqline(m1$residuals, col = "red")

##Data cleaning: I remove Addicted Score since it has the highest VIF value and there are other variables that likley capture what is happening within Addicted Score.

cleandata<- rawdata%>%
  select(!Addicted_Score)

```

## Fitting the Linear Model after removing Addicted Score (2/3)

I see that linearity, and normality are still out of line. I then remove sleep hours per night because it had a noisy distribution earlier, and I hope that the normality and linearity would improve with it removed.

```{r Refitting LM with new dataset transformations, fig.height=4, fig.width=6, paged.print=TRUE}

#rerun the model to see if multicolinearity improved
m2 <- lm(Avg_Daily_Usage_Hours ~ Age + Gender + Country + Academic_Level + 
              Most_Used_Platform + Affects_Academic_Performance + 
              Sleep_Hours_Per_Night + Mental_Health_Score + 
              Relationship_Status + Conflicts_Over_Social_Media,
            data = cleandata)
summary(m2)

##Checking for residual independence | PASS

dwtest(m2)


#collinearity? |PASS
vif(m2) 


## Checking for Normality of residuals | FAIL

qqnorm(m2$residuals)
qqline(m2$residuals, col = "red")

#Linearity of residuals | FAIL, angled and vertical is not good, even for Discrete variables

plot(m2$fitted.values, resid(m2))
abline(h = 0, col = "red")

# Loop through all continuous predictors
for (varname in numeric_vars) {

  if (varname != "Avg_Daily_Usage_Hours") {
    plot(rawdata[[varname]], resid(m2),
         main = paste("Residuals vs", varname))
    abline(h = 0, col = "red")
  }
}


##Removing Sleep_hours_per_night since the residual plot does not indicate a straightforward transformation to fix it.
cleandata<- cleandata%>%
  select(!Sleep_Hours_Per_Night)


```

## I fit the model a third time (3/3)

The removal of sleep hours fixes the skew in the residuals, but the vertical pattern persists. This is likely due to the discrete variables I have, which may or may not violate linearity assumptions if the discrete variable increments represent change at consistent increments as the ones I include do. If the discrete variables were factors, that would be a worse fit.

```{r Refitting the LM a third time with new transformations, fig.height=4, fig.width=6, paged.print=TRUE}

m3 <- lm(Avg_Daily_Usage_Hours ~ Age + Gender + Country + Academic_Level + 
              Most_Used_Platform + Affects_Academic_Performance + Mental_Health_Score + Relationship_Status + Conflicts_Over_Social_Media,
            data = cleandata)

##Checking residuals | UNSURE: Better but still vertical. perhaps that is just due to the Discrete variables I have

plot(m3$fitted.values, resid(m3))
abline(h = 0, col = "red")

## Checking for Normality of residuals | PASS

qqnorm(m3$residuals)
qqline(m3$residuals, col = "red")

##Checking for multicolinearity | PASS
vif(m3)

#Checking for residual independence | FAIL
dwtest(m3)

## Switching to a mixed effects model to see if country and mental health score are random effects.
```

## Switching to a mixed-effects model with bootstrapping

I switch to mixed effects to see if it improves the non-independence of the observations. **I put country as a random effect**, and tried various other combinations of variables to potentially address non-independence, but none improved the durbin watson test values enough to fail to reject the null hypothesis of independence. Mental_Health_Score as a random slope improved model performance, but worsened the tails on the qq plot (not shown), so it was again removed as a random effector. I tried many variables in addition to country to improve non-independence, but none worked. **I decide to use the mixed effects model with bootstrapped confidence intervals** since the mixed effects model minimized the BIC in the model iterations I had with fewer confidence intervals compared to the LMs, and I wanted to increase the chance of seeing significant relationships since bootstrapped confidence intervals can be conservative. Bootstrapping allows for confidence intervals that do not depend on parametric assumptions, and since we have abnormal linearity and non-independence, bootstrapping is the right choice.

```{r Switching to Mixed Effects Model to Try and Address Non-Independence, fig.height=4, fig.width=6, paged.print=TRUE}


m4 <- lmer(Avg_Daily_Usage_Hours ~ Age + Gender + Academic_Level + 
           Most_Used_Platform + Affects_Academic_Performance + 
           Mental_Health_Score + Relationship_Status + 
           Conflicts_Over_Social_Media + 
           (1 | Country),
           control = lmerControl(optimizer = "bobyqa"),
           data = cleandata)
summary(m4)

##Comparing performance between m3 and m4 | the mixed effects model (m4) has a substantially lower BIC.
Anova(m3)
Anova(m4)

BIC(m3,m4)


## Checking for Normality and linearity of residuals | UNSURE Normality looks ok, with some caveats. Discrete variables are likely causing the pattern we see, but I feel comfortable that this meets the normality assumption given how big the dataset is and that the Discrete variables we have are equally spaced and best kept as discrete. Linearity is good, with random scatter around zero. 

plot(fitted(m4), resid(m4)) 
abline(h = 0, col = "red")

qqnorm(resid(m4))
qqline(resid(m4), col = "red")

##Checking for multicolinearity | PASS
vif(m4)

## Checking Homoscedasticity | PASS
bp_model <- lm(resid(m4) ~ fitted(m4))
summary(bp_model)

#Checking for residual independence with an ACF plot | FAIL There is still autocorrelation.
acf(resid(m4), main = "ACF of Residuals (m4)")


#After trying various combinations of random and fixed effects after this point, I was unable to address the non-independence in the residuals. This model meets assumptions for linearity (with the caveat of the vertical lines), normality, multicollinearity, and homoscedasticity, but fails to meet the assumptions for independence of observations. Therefore I will be conducting hypothesis tests and constructing confidence intervals using bootstrapping which will allow for more robust inference in the presence of non-independence and non-linearity concerns.




```

## Can Lasso improve model performance?

I will now address the following: Implement at least two different **variable selection** techniques (Lasso and Elastic Net), **assess model performance** with RMSE and MAE ( I also compared linear models with BIC earlier), and I will validate my chosen model using **cross validation.**

First, I will conduct variable selection using **lasso and use cross validation** to determine the optimal lambda. LASSO drops a couple dummy variables, but does not drop any variable outright.

```{r Performing Variable Selection with Lasso, fig.height=4, fig.width=6, paged.print=TRUE}



# Create a model matrix for the predictor variables
x <- model.matrix(Avg_Daily_Usage_Hours ~ Age + Gender + Academic_Level + 
                  Most_Used_Platform + Affects_Academic_Performance + 
                  Sleep_Hours_Per_Night + Mental_Health_Score + 
                  Relationship_Status + Conflicts_Over_Social_Media + Country, 
                  data = rawdata)[, -1]  # Remove the intercept

# Define the target variable
y <- rawdata$Avg_Daily_Usage_Hours

# Fit the Lasso model with cross-validation to select the optimal lambda
lasso_model <- cv.glmnet(x, y, alpha = 1) 

# Plot the cross-validation results to see the lambda values
plot(lasso_model)

# Get the lambda that minimizes complexity within one se of the lowest CV error
LASS_best_lambda <- lasso_model$lambda.1se

# Get the coefficients corresponding to the best lambda
LASScoefficients <- coef(lasso_model, s = "lambda.1se")
print(LASScoefficients)

# Fit the best lasso model
lasso_best_model <- glmnet(x, y, alpha = 1, lambda = LASS_best_lambda)





```

## What about Elastic Net?

Will it help address correlated predictors and improve model performance? Similar to Lasso, Elastic net drops some dummy variables in Country and Most Used Platform, but no full variable classes.

```{r Performing Variable Selection with Elastic Net, fig.height=4, fig.width=6, paged.print=TRUE}

# Fit the Lasso model with cross-validation to select the optimal lambda
elastic_model <- cv.glmnet(x, y, alpha = 0.5) 

# Plot the cross-validation results to see the lambda values
plot(elastic_model)

# Get the lambda that minimizes complexity within one se of the lowest CV error
ELAST_best_lambda <- elastic_model$lambda.1se

# Get the coefficients corresponding to the best lambda
ELASTcoefficients <- coef(elastic_model, s = "lambda.1se")
print(ELASTcoefficients)

# Fit the best lasso model
ELAST_best_model <- glmnet(x, y, alpha = 1, lambda = ELAST_best_lambda)
```

## Cross Validation of the Mixed Effects Model

Below, I complete cross validation to compare the performance of the mixed effects model to the models from Lasso and Elastic Net.

```{r Performing Cross Validation with the MM to compare to LASSO and ELASTIC}


set.seed(1)

# Number of folds for cross-validation
k <- 10

# Create folds for cross-validation
folds <- sample(rep(1:k, length.out = nrow(cleandata)))

# Initialize vectors to store results
m4rmse <- numeric(k)
m4mae <- numeric(k)

# Cross-validation loop
for (i in 1:k) {
  # Indices for the test set
  test_idx <- which(folds == i)
  
  # Create training and testing sets (80% training, 20% testing for each fold)
  train_data <- cleandata[-test_idx, ]
  test_data <- cleandata[test_idx, ]
  
  # Fit the mixed-effects model on the training set
  m4 <- lmer(Avg_Daily_Usage_Hours ~ Age + Gender + Academic_Level + 
             Most_Used_Platform + Affects_Academic_Performance + 
             Mental_Health_Score + Relationship_Status + 
             Conflicts_Over_Social_Media + 
             (1 | Country), 
             data = train_data,
            control = lmerControl(optimizer = "bobyqa"))
  
  # Predict on the test set
  y_true <- test_data$Avg_Daily_Usage_Hours
  y_pred <- predict(m4, newdata = test_data, allow.new.levels = TRUE)
  
  # Compute RMSE and MAE for the current fold
  m4rmse[i] <- sqrt(mean((y_pred - y_true)^2, na.rm = TRUE))
  m4mae[i] <- mean(abs(y_pred - y_true), na.rm = TRUE)
}


```

## Comparing the RMSE and MAE of the Mixed-Effect, Lasso, and Elastic Net models

The results of comparing the RMSE and MAE values of the training data suggest that the variable selection of LASSO and Elastic Net could lead to more promising model performance on testing data. The Lasso model minimized RMSE and MAE the lowest, with elastic net being the second lowest RMSE and MAE. However, the Mixed-Effects model still performs well, and importantly and it is more straightforward for me to apply bootstrapping the mixed-effects model fit (which I would like to demonstrate) than it would be to integrate the dummy variables from LASSO and Elastic Net.

```{r MM, LASSO & ELASTIC Metric Comparison}



# Compute RMSE and MAE for LASSO Algorithm

#Calculate predictions using the fitted lasso model
LASSpredictions <- predict(lasso_best_model, s = "lambda.1se", newx = x)

LASSrmse <- sqrt(mean((y - LASSpredictions)^2))
LASSmae <- mean(abs(y - LASSpredictions))

# Compute RMSE and MAE for LASSO Algorithm

#Calculate predictions using the fitted lasso model
ELASTpredictions <- predict(ELAST_best_model, s = "lambda.1se", newx = x)

ELASTrmse <- sqrt(mean((y - ELASTpredictions)^2))
ELASTmae <- mean(abs(y - ELASTpredictions))

#Make Summary Table
summarytable <- data.frame(
  Model = c("LASSO", "Mixed Effects", "Elastic Net"),
  RMSE = c(LASSrmse, mean(m4rmse), ELASTrmse),
  MAE = c(LASSmae, mean(m4mae), ELASTmae))


print(summarytable) 

```

------------------------------------------------------------------------

# Discussion & Conclusions:

## Mixed-Effects Model: Feature Impact Analysis & Hypothesis Testing

In the final code block, I **quantify and interpret the impact of each feature on the target,** I **provide confidence intervals for significant coefficients,** I **explain the practical significance of my findings in the context of the dataset,** and I **perform hypothesis tests on coefficients.** The output of the code block provides the coefficient estimates and CI's of the variables that have 95% CI outside of zero.

## Results:

**The confidence intervals suggest some real effects of some variables on screen time, with many variables and categorical factors having regression coefficients with confidence intervals excluding zero.** That means that if we repeated this entire study multiple times, and constructed a 95% CI each time, 95% of those times the CI windows for variables and factors highlighted below would contain the true population coefficient. **My hypothesis** that some of these variables would have a significant association with average screen time was correct. The significant results are as follows:

-   Age has a small [0.044, 0.167] but significantly positive relationship with screen time, showing that each year of age is associated with a slight increase in daily social media usage.

-   Undergraduate students had a positive association with higher screen time [0.149, 0.545].

-   The most commonly used social media platform (Most_Used_Platform) has variable impacts depending on its level, with the two extremes being that those using mostly LinkedIn actually being associated with significantly less time on social media [-0.907, -0.236] compared to the reference platform of Facebook. Whatsapp users are associated with significantly more [0.499, 0.951] time on social media compared to Facebook.

-   Users who say that social media impacts their academic performance (Affects_Academic_PerformanceYes) actually are associated with significantly less social media use [-0.535, -0.038], perhaps because they are already conscious of their screen time.

-   Mental health score was negatively associated with screen time [-0.587, -0.347], meaning that the higher someone rated their mental health, the less screen time they are expected to have.

-   Lastly, people who have conflicts on social media tend to spend more time on social media [0.472, 0.781].

## Conclusions:

The practical significance of my findings are that the included significant variables are variables in almost everyone's life, meaning that if someone is interested in decreasing their own relationship with screen time, the results of this model could indicate to someone if their actions or perspectives on screen time indicate something about their relationship with average daily screen use. For example, if someone is wondering if they should delete whatsapp or linkedIn to decrease screen time, they could use these results to make the decision to delete whatsapp, since linkedIn is actually associated with a decrease in screen time. Undergraduate education is often a time when people spend lots of time engaging with the cultural zeitgeist and exploring socially, so it is also no surprise to see that there is a positive increase in screen use per day for this demographic.

```{r Feature Comparison, CI, Significance, Hypothesis Tests}

#Completing hypothesis tests and estimates using the mixed effects model and bootstrapping for confidence intervals
set.seed(1)
# Define function to extract fixed effect estimates
getFixef <- function(model) {
  fixef(model)
}

# Perform bootstrapping using a function appropriate for mixed models
boot_results <- bootMer(m4, FUN = getFixef, nsim = 1000)

# Compute percentile CIs
boot_cis <- confint(boot_results)

# Get fixed effect estimates
coef_estimates <- fixef(m4)

# Create a tibble of CIs and coefficient estimates
boot_cis_sig <- as_tibble(boot_cis, rownames = "Variable") |>
  rename(Lower_CI = `2.5 %`, Upper_CI = `97.5 %`) |>
  filter((Lower_CI > 0 & Upper_CI > 0) | (Lower_CI < 0 & Upper_CI < 0)) |>
  mutate(
    Coefficient = coef_estimates[Variable],
    Coefficient = round(Coefficient, 3),
    Lower_CI = round(Lower_CI, 3),
    Upper_CI = round(Upper_CI, 3)
  )

print(boot_cis_sig)

```

# References:

-   **ACF Plot Guide**

    <https://koalatea.io/r-plotting-acf-timeseries/>

-   **Biometry II Class Notes (Course resources listed below)**

    <https://nayelbettache.github.io/STSCI6020.html>

-   **Car reference manual**\
    <https://cran.r-project.org/web/packages/car/index.html>

-   **Dataset**

    <https://www.kaggle.com/datasets/adilshamim8/social-media-addiction-vs-relationships/data>

-   **Glmnet reference manual**\
    <https://cran.r-project.org/web/packages/glmnet/index.html>

-   **Leaps reference manual**\
    <https://cran.r-project.org/web/packages/leaps/index.html>

-   **Lme4 reference manual**\
    <https://cran.r-project.org/web/packages/lme4/index.html>

-   **Lmtest reference manual**\
    <https://cran.r-project.org/web/packages/lmtest/index.html>

-   **Skimr reference manual**\
    <https://cran.r-project.org/web/packages/skimr/index.html>

-   **Tidyverse reference manual**\
    <https://cran.r-project.org/web/packages/tidyverse/index.html>
